<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="A.I.C.O">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="A.I.C.O">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A.I.C.O">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>A.I.C.O</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A.I.C.O</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/31/循环神经网络（RNN）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="目录">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A.I.C.O">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/31/循环神经网络（RNN）/" itemprop="url">循环神经网络（RNN）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-31T21:21:09+08:00">
                2019-05-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h1><p>在了解隐马尔可夫模型（HMM）之后，我们发现，通过统计的方法可以去观察和认识一个事件序列上临近事件发生的概率转化问题。在RNN模型中是允许模型在训练中取学期前后之间的转化影响的，只不过就是在RNN模型中你无法得到那种标准的HMM模型训练中得到的清晰的转化矩阵</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>传统的RNN从外形上来看就是这样一个结构。 下面这个 Vector 是输入向量，我们称之为 X，右侧的Y是输出向量，在使用 的过程中和以前我们看过的卷积网络去做图片分类一样一一卷积网络中的图片分类在训练的过程中实际上是把所有的样本和标签一对 一对放入网络，图片在“入口”，分类标签在“出口”，用样本图片产生的拟合值与“出口”标签的差来定义残差，然后一步一步去挪动网络中的各种权重 w, 使得残差向着减小的方向前进。 在RNN中的计算方式没有什么大的差别，向量 X，放在人 口的位置，待输出的内容放在 Y的位置。 中间单元（就是图中无字的白方框）中的待定权重 一旦设定，就一定会产生一个残差。<br><img src="/2019/05/31/循环神经网络（RNN）/./1559305821069.png" alt="Alt text"><br>这里有两个待定系数 一个是$W_x$， 一个是 $W_h$，其中$W_x$会与$W_t$向量做乘积，作为输入的一部分，那么另一部分呢，是由前一次输出的 $H_{t-1}$， 和 $W_H$相乘得到。 等于说前一次计算输 出的 $H_t-1$， 需要缓存一下，在本次$X_.t$; 输入的时候参与运算，共同输出最后的$Y$。 而 $Y$也是一个向量，它是由前面输入的 $H_t-1$和$W_H$相乘所产生的向量$W_x$和$X_t$相乘所产生的向量加和后做 <strong>SOFTMAX</strong> 得到的。 <strong>SOFTMAX</strong> 我们是知道的，输出的是一个多维向量，不论有多少个分量，其加和都是 1 ，每个向量的分量维度是一个小于 1 的值，而这个值是可以做概率解释的。</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>最简单的RNN模型在工作的时候是可以以一个单元工作的，从时间维度上做一个展开可以像下面这样画：<br><img src="/2019/05/31/循环神经网络（RNN）/./1559306712483.png" alt="Alt text"><br>在这个训练过程中，文字是无法直接扔进去的，都会通过一个“word to vector”的功能模块将文字转化为数字向量。</p>
<p>当$W_x$和$W_H$这两个矩阵被初始化之后，一定是在Y侧有输出的，所以一定会有残差产生。设每个样本产生的残差为$E_i$在一次完整的训练中，整个网络的残差就是从第一次扔进去对话的第一句和第二句的时候产生的$E_1$。然后加上第二句和第三句产生的$E_2$。。。所以可以简写成：</p>
<blockquote>
<p>$Loss =  \sum_{i = 1}^{n-1}E_i$</p>
</blockquote>
<p>但是整个误差并没有这么简单。</p>
<h2 id="误差传递"><a href="#误差传递" class="headerlink" title="误差传递"></a>误差传递</h2><p>假定整个网络确实有那么一个状态，此时使得$W_x$和$W_H$的值能够满足残差总和最小的 情况。 那么在我初始化$W_x$和 $W_H$的时候我仍然需要把这两个矩阵中的各维度分量值向着减 小残差的方向去移动，方向是好确定的一一我们说过，最土的方法可以向正方向挪动一个 很小的值，然后向负方向挪动一个很小的值，比较一下哪一种产生的残差和更小。 而我们前面多次见过的用链式法则求导的过程，目的是为了确定挪多少量更合适，没错吧？ 在当前这个网络模型中 $W_x$和 $W_H$都是我们最终要学习的内容，其实残差总和$Loss$应该 来自于两个部分： 一部分是由于 $W_x$和理想的 $W_x$的状态的差距造成的，而另一部分是 $W_H$和理想的 $W_H$的状态差距造成的。 现在就是要求出关于这两个向量的导数一一一也就是斜率的 表达式来确定每次移动多少。 </p>
<p>对于整个网络的误差两部分来源，我们写成这样：</p>
<blockquote>
<p>$Loss =  aE_x + bE_H$</p>
</blockquote>
<p>其中$E_x$表示由$W_x$引起的误差，$E_H$表示由$W_H$引起的误差，a和b 分别表示由样本产生的系数。所以我们展开一下：</p>
<blockquote>
<p>$H_T = W_Hf ( H_{t-1}  ) + W_xX_t$<br>$Y_t = SOFTMAX(f(H_t))$</p>
</blockquote>
<p>$SOFTMAX$函数：</p>
<blockquote>
<p>$D = MAX(V)$<br>$S_i = \frac{e^{V_i-D}}{\sum_{i}^{c}e^{V_i-D}}$</p>
</blockquote>
<p>如果只有一对输入$X_t$和$Y_t$，也就是$X_1$和$Y_1$，那么这个时候残差是什么？</p>
<blockquote>
<p>$H_{1}^{o} = W_Hf()+W_xX_1$<br>$E_1 = \frac{1}{2}(SOFTMAX(f(H_{1}^{0}))-Y_1)^2$</p>
</blockquote>
<p>也就是</p>
<blockquote>
<p>$E_1 = \frac{1}{2}(W_S(f(H_{1}^{0}))-Y_1)^2$</p>
</blockquote>
<p>$W_s$是指$SOFTMAX$中的$W_s$矩阵</p>
<p>后面就不管了，总之，当训练过程中有成千上万个向量放入网络后，要计算这么多向量产生误差的导数大小，基本上不太现实。不仅是时间复杂度的问题，还有可能引发梯度消失或者梯度爆炸的问题<br>因此，传统的RNN虽然在理论上行得通，但是在训练中效果十分不理想，所以我们用一种叫做LSTM的算法代替BPTT算法来实现RNN的训练方式</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/28/隐马尔可夫模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="目录">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A.I.C.O">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/28/隐马尔可夫模型/" itemprop="url">隐马尔可夫模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-28T21:56:37+08:00">
                2019-05-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h1><h2 id="什么是熵"><a href="#什么是熵" class="headerlink" title="什么是熵"></a>什么是熵</h2><p>简单来说，熵是表示物质系统状态的一种度量，用它老表征系统的无序程度。熵越大，系统越无序，意味着系统结构和运动的不确定和无规则；反之，，熵越小，系统越有序，意味着具有确定和有规则的运动状态。熵的中文意思是热量被温度除的商。负熵是物质系统有序化，组织化，复杂化状态的一种度量。</p>
<p>熵最早来原于物理学. 德国物理学家鲁道夫·克劳修斯首次提出熵的概念，用来表示任何一种能量在空间中分布的均匀程度，能量分布得越均匀，熵就越大。<br> 一滴墨水滴在清水中，部成了一杯淡蓝色溶液<br> 热水晾在空气中，热量会传到空气中，最后使得温度一致</p>
<p>更多的一些生活中的例子:<br> 熵力的一个例子是耳机线，我们将耳机线整理好放进口袋，下次再拿出来已经乱了。让耳机线乱掉的看不见的“力”就是熵力，耳机线喜欢变成更混乱。<br>    熵力另一个具体的例子是弹性力。一根弹簧的力，就是熵力。 胡克定律其实也是一种熵力的表现。<br>    万有引力也是熵力的一种(热烈讨论的话题)。</p>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>简单来说，消息产生的种类越多，概率越均等则信息熵越大，反之，消息产生而定种类越是单一，概率产生越偏向其中某一个信息，那么熵值越小。</p>
<p>我们先上个公式看看：</p>
<p>$H\left ( x \right ) = -\sum_{i=1}^{n}p\left ( x_i \right )log_2P\left ( x_i \right )$</p>
<p>这个就是一个信息熵的求法公式，这里log是以2 为底的，但是其实也可以以其他的数值为底，但是在某一次应用的整个过程中，参加本次应用的所有信息熵都必须要采用同一个第，不能将不同底的对数求出的熵再做加和或者比较，不然这样完全没有意义</p>
<h2 id="HMM-隐马尔可夫模型"><a href="#HMM-隐马尔可夫模型" class="headerlink" title="HMM(隐马尔可夫模型)"></a>HMM(隐马尔可夫模型)</h2><p>隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。</p>
<p>是在被建模的系统被认为是一个马尔可夫过程与未观测到的（隐藏的）的状态的统计马尔可夫模型。</p>
<p>下面用一个简单的例子来阐述：</p>
<p>假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。<br><img src="/2019/05/28/隐马尔可夫模型/./1558592688922.png" alt="Alt text"><br>假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4</p>
<p>这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8</p>
<p>一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。</p>
<p>同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。<br><img src="/2019/05/28/隐马尔可夫模型/./1558592735097.png" alt="Alt text"><br>其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。</p>
<p>以上只是一个比较简单易懂的例子</p>
<hr>
<p>HMM模型相关的算法主要分为三类，分别解决三种问题：<br>      <strong>1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。</strong><br>      这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2.</p>
<p><strong>2）还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。</strong><br>      看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。</p>
<p><strong>3）知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。</strong><br>      这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。</p>
<p>任何一个HMM都可以通过下列五元组来描述：</p>
<ul>
<li>param obs:观测序列</li>
<li>param states:隐状态</li>
<li>param start_p:初始概率（隐状态）</li>
<li>param trans_p:转移概率（隐状态）</li>
<li>param emit_p: 发射概率 （隐状态表现为显状态的概率）</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/13/VGG-16网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="目录">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A.I.C.O">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/13/VGG-16网络/" itemprop="url">VGG-16网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-13T21:05:32+08:00">
                2019-05-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="VGG-16网络"><a href="#VGG-16网络" class="headerlink" title="VGG-16网络"></a>VGG-16网络</h1><h2 id="什么是VGG"><a href="#什么是VGG" class="headerlink" title="什么是VGG"></a>什么是VGG</h2><p>简单来说就是一个著名的卷积网络，16是指有16个带有参数的网络层，是一个带有完整的卷积层，池化层 ，全连接层的神经网络。</p>
<h2 id="如何看懂VGG-16"><a href="#如何看懂VGG-16" class="headerlink" title="如何看懂VGG-16"></a>如何看懂VGG-16</h2><p>首先我们看看官方给的文档<br><img src="/2019/05/13/VGG-16网络/./20170325221311877.png" alt="Alt text"><br>龟龟，完全看不懂，我只知道conv3-128在此处表示是128个3*3的卷积，然后这些表示什么我也不懂，所以不愧是官方的文档，告辞。</p>
<p>然后我找到了一个整个VGG-16的架构图<br><img src="/2019/05/13/VGG-16网络/./`]QFZD7GRARRHXBXGEPKVJ.png" alt="Alt text"><br>从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。<br>可以看到 VGG16 是13个卷积层+3个全连接层叠加而成</p>
<p>好，这个时候我们就会有点蒙蔽，首先一张图他特么的怎么是个三维的？好的这个问题我也不知道为什么。但是就先这么认为他就是一个三维的。然后我们进入卷积层，他有64个3<em>3</em>3的卷积核(书上说的是3<em>3的卷积核，但是如果是3</em>3这就不对了感觉)然后我们扫完这个矩阵，就得到了64层矩阵。话不多说先贴个图<br><img src="/2019/05/13/VGG-16网络/./1557752177267.png" alt="Alt text"><br>我们假设蓝色框是一个RGB图像，橙色是一个3<em>3</em>3的卷积核，我们对一个三维的27个数求和，然后扫过去，按照第一部分算的得出来的是一维的298<em>298的矩阵（因为卷积核也是三维所以结果是一维）然后回想一下什么是Padding、前面也讲过它的概念了；所以不了一圈的圆，回到了300</em>300*1；</p>
<p>将就着理解一下为什么一个卷积核扫完就是一个一维矩阵，一共64个卷积核就形成了一个64层的矩阵</p>
<p>然后我们经过一个池化操作，小矩阵是(2,2) ，步长(2,2),指的是横向每次移动2格，纵向每次移动2格这样我们得到的矩阵就的宽高就是之前的一半。</p>
<p>再往下就同理，只不过是卷积核个数依此变为128，256，512，每次池化之后，矩阵都要缩小一半。</p>
<p>13层卷积和池化之后，数据就变成了512<em>7</em>7</p>
<p>有不懂后续再做补充。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/09/卷积网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="目录">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A.I.C.O">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/09/卷积网络/" itemprop="url">什么是卷积网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-09T17:32:03+08:00">
                2019-05-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h1><p>##卷积层</p>
<p>我们先看卷积核的表达方式</p>
<blockquote>
<p>f(x) = wx + b</p>
</blockquote>
<p>然后简单了解一下卷积层的几个名词</p>
<ul>
<li>深度/depth</li>
<li>步长/stride</li>
<li>填充值/padding<br><img src="/2019/05/09/卷积网络/./深度和步长.png" alt="Alt text"></li>
</ul>
<p>填充值是什么呢？以下图为例子，比如有这么一个5<em>5的图片（一个格子一个像素），我们滑动窗口取2</em>2，步长取2，那么我们发现还剩下1个像素没法滑完，那怎么办呢？<br><img src="/2019/05/09/卷积网络/./填充值1.png" alt="Alt text"><br>那我们在原先的矩阵加了一层填充值，使得变成6*6的矩阵，那么窗口就可以刚好把所有像素遍历完。这就是填充值的作用。<img src="/2019/05/09/卷积网络/./填充值2.png" alt="Alt text"></p>
<hr>
<h3 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h3><p>接下来我们看看卷积核<br><img src="/2019/05/09/卷积网络/./卷积核1.png" alt="Alt text"><br>如图，现有一张图片有5×5个像素点，每个像素点只有0和1，然后提取此图片的特征，首先我们设计一个简单的卷积核，假设神经元</p>
<blockquote>
<p>w = [1,1,1,1,1,1,1,1,1]<br>b = [0]</p>
</blockquote>
<p>w 由 9 个1 组成，在此场景里，我们指用黄色部分这个3×3的方块来从左到右从上到下这9个点作为x向量与w相乘完成内积操作，并与 b 相加,整个过程就是这样:</p>
<blockquote>
<p>f(x) = 1×1 + 1×1 + 1×1 + 1×0 + 1×1 + 1×1 + 1×0 + 1×0 +1×1+ 0 = 6</p>
</blockquote>
<hr>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>那么左上角这个黄色小方框就会输出一个6，我们把这个6单独存入一个存储空间。了解了简单的计算方式我们看看稍微复杂点的卷积层<br><img src="/2019/05/09/卷积网络/./卷积核2.png" alt="Alt text"><br>这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。<img src="/2019/05/09/卷积网络/./卷积核3.png" alt="Alt text"><br>蓝色的矩阵(输入图像)对粉色的矩阵（filter）进行矩阵内积计算并将三个内积运算的结果与偏置值b相加（比如上面图的计算：2+（-2+1-2）+（1-2-2） + 1= 2 - 3 - 3 + 1 = -3），计算后的值就是绿框矩阵的一个元素。</p>
<p>以下展示一下卷积层的计算过程<br><img src="/2019/05/09/卷积网络/./卷积核4.gif" alt="Alt text"></p>
<h2 id="激励层"><a href="#激励层" class="headerlink" title="激励层"></a>激励层</h2><p>激励层就是把卷积层输出结果做非线性映射<br><img src="/2019/05/09/卷积网络/./激励层.png" alt="Alt text"></p>
<p>CNN(卷积网络)采用的激励函数一般为ReLU(y  = max (0,x)),它的特点是收获快，求梯度简单，但是比较脆弱<br><img src="/2019/05/09/卷积网络/./ReLU.png" alt="Alt text"></p>
<ul>
<li>不要使用sigmod(前馈网络常用的激励函数)</li>
<li>首先试ReLU</li>
<li>次选Leaky ReLU 或者Maxout</li>
<li>有时候Tanh(双曲正切)也可以</li>
</ul>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。<br>简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。</p>
<p>这里再展开叙述池化层的具体作用。</p>
<ol>
<li><p>特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</p>
</li>
<li><p>特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。</p>
</li>
<li><p>在一定程度上防止过拟合，更方便优化。<br><img src="/2019/05/09/卷积网络/./池化层1.png" alt="Alt text"><br>池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。一个做了最大化一个做了平均化。</p>
</li>
</ol>
<p>这里就说一下Max pooling，其实思想非常简单。<br><img src="/2019/05/09/卷积网络/./池化层2.png" alt="Alt text"><br>对于每个2<em>2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2</em>2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。</p>
<p>同理 verage pooling 就是将前面输出过来的数据做一个取平均值的操作，比如以stride = 2 的 2×2 为Mean Pooling Filter 。输出的元素就是对应的平均值</p>
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的：</p>
<h2 id="一般CNN结构"><a href="#一般CNN结构" class="headerlink" title="一般CNN结构"></a>一般CNN结构</h2><ol>
<li>INPUT</li>
<li>[[CONV -&gt; RELU]<em>N -&gt; POOL?]</em>M </li>
<li>[FC -&gt; RELU]*K</li>
<li>FC</li>
</ol>
<h2 id="训练算法"><a href="#训练算法" class="headerlink" title="训练算法"></a>训练算法</h2><ol>
<li>同一般机器学习算法，先定义Loss function，衡量和实际结果之间差距。</li>
<li>找到最小化损失函数的W和b， CNN中用的算法是SGD（随机梯度下降）</li>
</ol>
<p>##典型CNN网络</p>
<ul>
<li>LeNet，这是最早用于数字识别的CNN</li>
<li>AlexNet， 2012 ILSVRC比赛远超第2名的CNN，比</li>
<li>LeNet更深，用多层小卷积层叠加替换单大卷积层。</li>
<li>ZF Net， 2013 ILSVRC比赛冠军</li>
<li>GoogLeNet， 2014 ILSVRC比赛冠军</li>
<li>VGGNet， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/07/tensorflow首次模拟/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="目录">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A.I.C.O">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/07/tensorflow首次模拟/" itemprop="url">tensorflow首次模拟</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-07T20:51:03+08:00">
                2019-05-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>@<a href="TENSORFLOW首次实践">TOC</a></p>
<h1 id="学习tensorflow的提前准备"><a href="#学习tensorflow的提前准备" class="headerlink" title="学习tensorflow的提前准备"></a>学习tensorflow的提前准备</h1><p>在学习使用tensorflow之前磕了半天的神经网络基础，然后开始首次实践手写板功能</p>
<h2 id="tensorflow的安装"><a href="#tensorflow的安装" class="headerlink" title="tensorflow的安装"></a>tensorflow的安装</h2><p>在cmd 中 用python语言安装即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow</span><br></pre></td></tr></table></figure></p>
<p>注：tensorflow的运行前提是numpy库的存在，记得更新numpy库<br>我们可以尝试运行一下<br>eg：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">&gt;&gt;&gt;hello = tf.constant(<span class="string">'hello,tensorflow'</span>)</span><br><span class="line">&gt;&gt;&gt;sess = tf.Session()</span><br><span class="line">&gt;&gt;&gt;print(sess.run(hello))</span><br><span class="line">hello,tensorflow</span><br></pre></td></tr></table></figure></p>
<p>当没有报错时，说明我们tensorflow安装成功</p>
<h2 id="tensorboard"><a href="#tensorboard" class="headerlink" title="tensorboard"></a>tensorboard</h2><p>tensorboard是一个可以让我们在训练网络的过程中通过仪表盘看到网络目前状态的表现情况的组件<br>在下载tensorflow时会一同打包下载<br>启动tensorboard需要在tensorflow环境下</p>
<p>#启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=(logs)        括号内为logs目录所在路径</span><br></pre></td></tr></table></figure></p>
<h2 id="MNIST数据集下载"><a href="#MNIST数据集下载" class="headerlink" title="MNIST数据集下载"></a>MNIST数据集下载</h2><p>MNIST是网上著名的公开数据集之一。官方网站在<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a><br>我们从上面下载这4个数据文件</p>
<blockquote>
<p>train-images-idx3-ubyte.gz:  training set images (9912422 bytes)<br>train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)<br>t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)<br>t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)</p>
</blockquote>
<h1 id="使用tensorflow完成实验"><a href="#使用tensorflow完成实验" class="headerlink" title="使用tensorflow完成实验"></a>使用tensorflow完成实验</h1><p>我们首先从<strong>tensorflow</strong> 的官方 <strong>github</strong> 上 clone一哈<br>地址在<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow</a><br>不得不说真的多，我克隆下来就用了半天<br>我们所用的文件目录在 tensorflow/tensorflow/examples/tutorials/mnist/中</p>
<p>将这个minst文件夹放在一个名为tensorflow的文件夹内，因为在<strong>fully_connected_feed.py</strong>文件中261行以及268行的两个目录索引为<strong>tensorflow/mnist/input_data</strong>和<strong>tensorflow/mnist/logs/fully_connected_feed</strong>。上一个表示训练数据集存放位置，所以我们从MNIST上下载的四个压缩包都放在<strong>input_date</strong>目录下,第二个是训练后生成的logs 所在位置，后面使用tensorboard的时候的logs目录即为此。</p>
<h2 id="第一个错误：Tensorflow-tensorflow-python-framework-errors-impl-NotFoundError-Failed-to-create-a-directory-tmp-tensorflow"><a href="#第一个错误：Tensorflow-tensorflow-python-framework-errors-impl-NotFoundError-Failed-to-create-a-directory-tmp-tensorflow" class="headerlink" title="第一个错误：Tensorflow-tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: /tmp\tensorflow"></a>第一个错误：Tensorflow-tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: /tmp\tensorflow</h2><p>抱着希望运行一哈,然后一大串warning之后一个报错。</p>
<blockquote>
<p>tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: /tmp\tensorflow</p>
</blockquote>
<p>这里大概意思就是代码中写的路径文件夹为<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里我们把代码中的```/tmp```中的```/```改成其他,比如```..```或者```.</span><br></pre></td></tr></table></figure></p>
<p>问题代码出现在260行和267行</p>
<h2 id="第二个错误：Tensorflow-lt-urlopen-error-WinError-10060"><a href="#第二个错误：Tensorflow-lt-urlopen-error-WinError-10060" class="headerlink" title="第二个错误：Tensorflow-&lt;urlopen error [WinError 10060]"></a>第二个错误：Tensorflow-&lt;urlopen error [WinError 10060]</h2><p>再次执行后继续报错</p>
<blockquote>
<p>urllib.error.URLError: <urlopen error [winerror 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。></urlopen></p>
</blockquote>
<p>这就是我之前没把数据集放在指定的<strong>tensorflow/mnist/logs/fully_connected_feed</strong>目录下然后报的错误，放进去之后我的<strong>fully_connected_feed.py</strong>文件成功运行。</p>
<h1 id="将训练完毕后的情况用tensorboard展现出来"><a href="#将训练完毕后的情况用tensorboard展现出来" class="headerlink" title="将训练完毕后的情况用tensorboard展现出来"></a>将训练完毕后的情况用tensorboard展现出来</h1><p>终于完成训练了，然后我们希望将它展现出来</p>
<h2 id="tensorboard-报错OSError-Errno-22-Invalid-argument"><a href="#tensorboard-报错OSError-Errno-22-Invalid-argument" class="headerlink" title="tensorboard 报错OSError: [Errno 22] Invalid argument"></a>tensorboard 报错OSError: [Errno 22] Invalid argument</h2><p>这里我僵硬了半天，百度谷歌一波之后告诉我是<strong>tensorboard</strong>没在<strong>tensorflow</strong>环境下运行</p>
<ul>
<li>解决方法一<br>执行以下代码：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate tensorflow</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>我之前又手贱安了anaconda这个没用几次的软件，导致环境不同。然后这个代码自动在anaconda里执行，然后报错，我一气之下就把它给删了<br>然后再运行。直接告诉我指令不存在。。。想必问题定不出在此处</p>
<ul>
<li>解决方法二</li>
</ul>
<p>修改manager.py 文件<br><img src="/2019/05/07/tensorflow首次模拟/$O(NN8ET(S]`{2P5XZ3~N$V.jpg" alt><br>按如下方式修改：<br><img src="/2019/05/07/tensorflow首次模拟/xiugai.jpg" alt><br>完成后执行，成功，破费！<br>输入<a href="http://localhost:6006" target="_blank" rel="noopener">http://localhost:6006</a>即可查看训练中的各种指标数据。在训练过程中的变化情况<br><img src="/2019/05/07/tensorflow首次模拟/20190419214313883.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">目录</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">目录</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
